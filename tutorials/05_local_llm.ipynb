{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b026fe8c",
   "metadata": {},
   "source": [
    "### 基礎設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c432534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 LangChain 1.0 串接本地 大模型\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen3:latest\",\n",
    "    temperature=0.5,\n",
    ")\n",
    "\n",
    "result = llm.invoke(\"What is the capital of the moon?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9359083",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05d60e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "\n",
    "prompt = \"请写一句关于『时间』的繁体中文，严格要求『8 到 15 个字』（不含标点），且文中绝对不能出现『时、间、流、逝、光、阴』这六个字。\"\n",
    "\n",
    "stream = chat(\n",
    "  model='qwen3:latest',\n",
    "  messages=[{'role': 'user', 'content': prompt}],\n",
    "  stream=True,\n",
    "  think=False\n",
    ")\n",
    "\n",
    "content = ''\n",
    "for chunk in stream:\n",
    "  if chunk.message.content:\n",
    "    print(chunk.message.content, end='', flush=True)\n",
    "    # accumulate the partial content\n",
    "    content += chunk.message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aba13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 使用 LangChain 1.0 實現"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13715ae1",
   "metadata": {},
   "source": [
    "### Thinking / Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a49774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "\n",
    "prompt = \"请写一句关于『时间』的繁体中文，严格要求『8 到 15 个字』（不含标点），且文中绝对不能出现『时、间、流、逝、光、阴』这六个字。\"\n",
    "\n",
    "stream = chat(\n",
    "  model='qwen3:latest',\n",
    "  messages=[{'role': 'user', 'content': prompt}],\n",
    "  stream=True,\n",
    "  think=True\n",
    ")\n",
    "\n",
    "in_thinking = False\n",
    "\n",
    "for chunk in stream:\n",
    "  if chunk.message.thinking and not in_thinking:\n",
    "    in_thinking = True\n",
    "    print('Thinking:\\n', end='')\n",
    "\n",
    "  if chunk.message.thinking:\n",
    "    print(chunk.message.thinking, end='')\n",
    "  elif chunk.message.content:\n",
    "    if in_thinking:\n",
    "      print('\\n\\nAnswer:\\n', end='')\n",
    "      in_thinking = False\n",
    "    print(chunk.message.content, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba1860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 使用 LangChain 1.0 實現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c474f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 使用 LangChain 1.0 實現\n",
    "# TODO: 補充 Reasoning Streaming 功能"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c475fb91",
   "metadata": {},
   "source": [
    "### Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135e515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Pet(BaseModel):\n",
    "  name: str\n",
    "  animal: str\n",
    "  age: int\n",
    "  color: str | None\n",
    "  favorite_toy: str | None\n",
    "\n",
    "class PetList(BaseModel):\n",
    "  pets: list[Pet]\n",
    "\n",
    "story = \"\"\"\n",
    "3歲的枯草色水豚阿呆正處於入定狀態，頭頂完美平衡著他最愛的玩具——一顆橘子。\n",
    "\n",
    "這份寧靜激怒了7歲的米白吉娃娃毀滅者。他發出尖銳的戰吼，把阿呆的後腿當成他最愛的玩具——別人的腳後跟，衝上去狠狠咬了一口。\n",
    "\n",
    "阿呆皮太厚完全沒感覺，只是緩慢地轉頭打了個哈欠。這微小的震動讓橘子滑落，精準地塞住了毀滅者正張大嘴狂吠的喉嚨。\n",
    "\n",
    "世界瞬間安靜了。\n",
    "\"\"\"\n",
    "\n",
    "response = chat(\n",
    "  model='qwen3:latest',\n",
    "  messages=[{'role': 'user', 'content': story}],\n",
    "  format=PetList.model_json_schema(),\n",
    ")\n",
    "\n",
    "pets = PetList.model_validate_json(response.message.content)\n",
    "print(pets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa60805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 使用 LangChain 1.0 實現"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359224a3",
   "metadata": {},
   "source": [
    "### Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65ff159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat \n",
    "\n",
    "# Tools\n",
    "def get_temperature(city: str) -> str:\n",
    "  \"\"\"Get the current temperature for a city\n",
    "  \n",
    "  Args:\n",
    "    city: The name of the city\n",
    "\n",
    "  Returns:\n",
    "    The current temperature for the city\n",
    "  \"\"\"\n",
    "  temperatures = {\n",
    "    'New York': '22°C',\n",
    "    'London': '15°C',\n",
    "  }\n",
    "  return temperatures.get(city, 'Unknown')\n",
    "\n",
    "# User Message\n",
    "messages = [{'role': 'user', 'content': \"New York 溫度多少?\"}]\n",
    "\n",
    "# Main Loop\n",
    "while True:\n",
    "  stream = chat(\n",
    "    model='qwen3:latest', # deepseek-r1:latest tool not supported issue\n",
    "    messages=messages,\n",
    "    tools=[get_temperature],\n",
    "    stream=True,\n",
    "    think=True,\n",
    "  )\n",
    "\n",
    "  thinking = ''\n",
    "  content = ''\n",
    "  tool_calls = []\n",
    "\n",
    "  in_thinking = False\n",
    "  in_content = False\n",
    "  in_tool_calls = False\n",
    "  # accumulate the partial fields\n",
    "  for chunk in stream:\n",
    "\n",
    "    # Thinking Part, In langchain it's an AIMessage\n",
    "    if chunk.message.thinking:\n",
    "      if not in_thinking:\n",
    "        in_thinking, in_content, in_tool_calls = True, False, False\n",
    "        print('\\nThinking:\\n', end='')\n",
    "      thinking += chunk.message.thinking\n",
    "      print(chunk.message.thinking, end='', flush=True)\n",
    "    \n",
    "    # Content Part, In langchain it's an HumanMessage\n",
    "    if chunk.message.content:\n",
    "      if not in_content:\n",
    "        in_thinking, in_content, in_tool_calls = False, True, False\n",
    "        print('\\nAnswer:\\n', end='')\n",
    "      content += chunk.message.content\n",
    "      print(chunk.message.content, end='', flush=True)\n",
    "    \n",
    "    # Tool Calls Part, In langchain it's an ToolMessage\n",
    "    if chunk.message.tool_calls:\n",
    "      if not in_tool_calls:\n",
    "        in_thinking, in_content, in_tool_calls = False, False, True\n",
    "        print('\\nTool Calls:\\n', end='')\n",
    "      tool_calls.extend(chunk.message.tool_calls)\n",
    "      print(chunk.message.tool_calls)\n",
    "\n",
    "  # append accumulated fields to the messages\n",
    "  if thinking or content or tool_calls:\n",
    "    messages.append({'role': 'assistant', 'thinking': thinking, 'content': content, 'tool_calls': tool_calls})\n",
    "\n",
    "  if not tool_calls:\n",
    "    break\n",
    "\n",
    "  # Execute the tool calls\n",
    "  for call in tool_calls:\n",
    "    if call.function.name == 'get_temperature':\n",
    "      result = get_temperature(**call.function.arguments)\n",
    "    else:\n",
    "      result = 'Unknown tool'\n",
    "    messages.append({'role': 'tool', 'tool_name': call.function.name, 'content': result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f74c49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 使用 LangChain 1.0 實現\n",
    "# TODO: Hint: 使用 create_agent API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c44118",
   "metadata": {},
   "source": [
    "### 練習\n",
    "\n",
    "* 測試不同的本地大模型\n",
    "* 多模態\n",
    "* Embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
